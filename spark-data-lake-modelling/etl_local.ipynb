{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Credentials and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "    \n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config.get(\"AWS\", \"AWS_ACCESS_KEY_ID\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config.get(\"AWS\", \"AWS_SECRET_ACCESS_KEY\")\n",
    "input_data = config.get(\"S3\", \"INPUT_DATA_LOCATION\")\n",
    "output_data = config.get(\"S3\", \"OUTPUT_DATA_LOCATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session with Hadoop AWS Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use latest hadoop package for EMR Cluster 3.3.0\n",
    "# Do you need this package for loading spark data frames from s3 ?\n",
    "spark = SparkSession.builder \\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Schema for Songs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date\n",
    "songSchema = R([\n",
    "    Fld(\"artist_id\",Str()),\n",
    "    Fld(\"artist_latitude\",Dbl()),\n",
    "    Fld(\"artist_location\",Str()),\n",
    "    Fld(\"artist_longitude\",Dbl()),\n",
    "    Fld(\"artist_name\",Str()),\n",
    "    Fld(\"duration\",Dbl()),\n",
    "    Fld(\"num_songs\",Int()),\n",
    "    Fld(\"song_id\",Str()),\n",
    "    Fld(\"title\",Str()),\n",
    "    Fld(\"year\",Int()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSongs = spark.read.option(\"recursiveFileLookup\",\"true\") \\\n",
    "                .json(\"./data/song_data\", schema=songSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "dfSongs.printSchema()\n",
    "dfSongs = dfSongs.withColumn(\"artist_name\",f.lower(f.col(\"artist_name\")))\n",
    "dfSongs = dfSongs.withColumn(\"title\",f.lower(f.col(\"title\")))\n",
    "dfSongs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer schema, fix header and separator : NOT POSSIBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfSongs = spark.read.option(\"recursiveFileLookup\",\"true\") \\\n",
    "#                 .json(\"./data/song_data\")\n",
    "# dfSongs.printSchema()\n",
    "# dfSongs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count of songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfSongs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Schema for Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, TimestampType as Timestamp, LongType as Long\n",
    "eventSchema = R([\n",
    "    Fld(\"artist\",Str()),\n",
    "    Fld(\"auth\",Str()),\n",
    "    Fld(\"firstName\",Str()),\n",
    "    Fld(\"gender\",Str()),\n",
    "    Fld(\"itemInSession\",Int()),\n",
    "    Fld(\"lastName\",Str()),\n",
    "    Fld(\"length\",Dbl()),\n",
    "    Fld(\"level\",Str()),\n",
    "    Fld(\"location\",Str()),\n",
    "    Fld(\"method\",Str()),\n",
    "    Fld(\"page\",Str()),\n",
    "    Fld(\"registration\",Dbl()),\n",
    "    Fld(\"sessionId\",Int()),\n",
    "    Fld(\"song\",Str()),\n",
    "    Fld(\"status\",Str()),\n",
    "    Fld(\"ts\",Long()),\n",
    "    Fld(\"userAgent\",Str()),\n",
    "    Fld(\"userId\",Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEvents = spark.read.json(\"./data/log-data/\", schema=eventSchema)\n",
    "dfEvents.printSchema()\n",
    "dfEvents = dfEvents.withColumn(\"artist\",f.lower(f.col(\"artist\")))\n",
    "dfEvents = dfEvents.withColumn(\"song\",f.lower(f.col(\"song\")))\n",
    "dfEvents.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEvents.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting date field to timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF Function for validating TS records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "@udf\n",
    "def parseTimestamp(ts):\n",
    "    from datetime import datetime\n",
    "    date_string = str(datetime.fromtimestamp(ts/1000))\n",
    "    return date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEvents = dfEvents.withColumn(\"ts\", parseTimestamp(\"ts\"))\n",
    "dfEvents.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert UserId to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, TimestampType as Timestamp, LongType as Long\n",
    "dfEvents = dfEvents.withColumn(\"userId\", dfEvents[\"userId\"].cast(Int()))\n",
    "dfEvents.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEvents.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building SONGS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table = dfSongs.select(\"song_id\", \"title\",\"artist_id\", \"year\", \"duration\")\n",
    "songs_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Artists Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_table_fields = [\"artist_id\", \"artist_name\",\"artist_location\", \"artist_latitude\", \"artist_longitude\"]\n",
    "artists_table_new_fields = [\"artist_id\", \"name\",\"location\", \"latitude\", \"longitude\"]\n",
    "artists_table_exprs = [ \"{} as {}\".format(oldField, newField) for (oldField, newField) in zip(artists_table_fields, artists_table_new_fields) ]\n",
    "\n",
    "artists_table = dfSongs.selectExpr(*artists_table_exprs).distinct()\n",
    "artists_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "users_table_fields = [\"userId\", \"firstName\",\"lastName\", \"gender\", \"level\"]\n",
    "users_table_new_fields = [\"user_id\", \"first_name\",\"last_name\", \"gender\", \"level\"]\n",
    "users_table_exprs = [ \"{} as {}\".format(oldField, newField) for (oldField, newField) in zip(users_table_fields, users_table_new_fields) ]\n",
    "users_table = dfEvents.selectExpr(*users_table_exprs).distinct()\n",
    "#users_table.limit(5).toPandas()\n",
    "#users_table.count() #107\n",
    "#users_table is showing user_id as double value why ?\n",
    "# distinct users_table contains NaN values as well.\n",
    "users_table_with_drop_duplicates = dfEvents.selectExpr(*users_table_exprs)\n",
    "ans = users_table_with_drop_duplicates.dropDuplicates([\"user_id\"])\n",
    "#ans.count() #98\n",
    "ans.groupBy(\"user_id\").count().orderBy(desc(\"count\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "time_table_fields = [\"ts\"]\n",
    "time_table_new_fields = [\"start_time\"]\n",
    "time_table_exprs = [ \"{} as {}\".format(oldField, newField) for (oldField, newField) in zip(time_table_fields, time_table_new_fields) ]\n",
    "time_table = dfEvents.selectExpr(*time_table_exprs).dropDuplicates([\"start_time\"])\n",
    "# There were 33 duplicate time records\n",
    "#time_table.count()\n",
    "time_table = time_table.withColumn(\"hour\", F.hour(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"day\", F.dayofweek(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"week\", F.weekofyear(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"month\", F.month(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"year\", F.year(\"start_time\"))\n",
    "# Clear parantheses for logical operators is necessary\n",
    "time_table = time_table.withColumn(\"weekday\", ((F.dayofweek(\"start_time\") > 0) & (F.dayofweek(\"start_time\") < 6)) )\n",
    "time_table.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SongPlays table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dfSongs = dfSongs.alias('dfSongs')\n",
    "#dfEvents = dfEvents.alias('dfEvents')\n",
    "dfEvents = dfEvents.filter(dfEvents.page == \"NextSong\")\n",
    "condition = (( dfEvents[\"artist\"] == dfSongs[\"artist_name\"]) & (dfEvents[\"song\"] == dfSongs[\"title\"]) & (dfEvents[\"length\"] == dfSongs[\"duration\"]) )\n",
    "songplays = dfEvents.join(dfSongs, condition, \"inner\").select(dfEvents[\"ts\"], dfEvents[\"userId\"], dfEvents[\"level\"], dfSongs[\"song_id\"], dfSongs[\"artist_id\"], dfEvents[\"sessionId\"], dfEvents[\"location\"], dfEvents[\"userAgent\"])\n",
    "songplays.limit(5).toPandas()\n",
    "#songplays.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning Ideas\n",
    "### There are event records which are invalid with year 0\n",
    "### Invalid timestamp where ValueError: year 50841 is out of range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
